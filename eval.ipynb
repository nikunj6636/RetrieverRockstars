{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating with N-grams metrics (ROUGE, BLEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Question',\n",
       " 'Source Docs',\n",
       " 'Question Type',\n",
       " 'Source Chunk Type',\n",
       " 'Answer',\n",
       " 'Generated Answers']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = \"new_questions_with_LLM_answers.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "columns = df.columns.tolist()\n",
    "\n",
    "columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "from typing import List, Dict\n",
    "\n",
    "def preprocess_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Preprocess text by converting to lowercase and tokenizing\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(str(text).lower())\n",
    "    return tokens\n",
    "\n",
    "def compute_bleu_score(reference: str, candidate: str) -> float:\n",
    "    reference_tokens = preprocess_text(reference)\n",
    "    candidate_tokens = preprocess_text(candidate)\n",
    "    \n",
    "    references = [reference_tokens]\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    \n",
    "    # Calculate BLEU score with equal weights for 1-4 grams\n",
    "    weights = (0.25, 0.25, 0.25, 0.25)\n",
    "    \n",
    "    return sentence_bleu(references, candidate_tokens, \n",
    "                           weights=weights,\n",
    "                           smoothing_function=smoothing)\n",
    "\n",
    "\n",
    "def compute_rouge_scores(reference: str, candidate: str) -> Dict[str, float]:\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    scores = scorer.score(str(reference), str(candidate))\n",
    "    \n",
    "    # Extract F1 scores\n",
    "    return {\n",
    "        'rouge1': scores['rouge1'].fmeasure,\n",
    "        'rouge2': scores['rouge2'].fmeasure,\n",
    "        'rougeL': scores['rougeL'].fmeasure\n",
    "    }\n",
    "\n",
    "def evaluate_qa_metrics(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute BLEU and ROUGE metrics for QA pairs in CSV file\n",
    "    \"\"\"\n",
    "    nltk.download('punkt')\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Initialize lists to store metrics\n",
    "    bleu_scores = []\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Get reference and generated answers\n",
    "        reference = row['Answer'] \n",
    "        generated = row['Generated Answers']\n",
    "        \n",
    "        bleu = compute_bleu_score(reference, generated)\n",
    "        bleu_scores.append(bleu)\n",
    "        \n",
    "        rouge_scores = compute_rouge_scores(reference, generated)\n",
    "        rouge1_scores.append(rouge_scores['rouge1'])\n",
    "        rouge2_scores.append(rouge_scores['rouge2'])\n",
    "        rougeL_scores.append(rouge_scores['rougeL'])\n",
    "    \n",
    "    # Add metrics to DataFrame\n",
    "    df['BLEU'] = bleu_scores\n",
    "    df['ROUGE-1'] = rouge1_scores\n",
    "    df['ROUGE-2'] = rouge2_scores\n",
    "    df['ROUGE-L'] = rougeL_scores\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    metrics_summary = {\n",
    "        'Average BLEU': df['BLEU'].mean(),\n",
    "        'Average ROUGE-1': df['ROUGE-1'].mean(),\n",
    "        'Average ROUGE-2': df['ROUGE-2'].mean(),\n",
    "        'Average ROUGE-L': df['ROUGE-L'].mean()\n",
    "    }\n",
    "    \n",
    "    print(\"\\nMetrics Summary:\")\n",
    "    for metric, value in metrics_summary.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nikunj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics Summary:\n",
      "Average BLEU: 0.0561\n",
      "Average ROUGE-1: 0.2936\n",
      "Average ROUGE-2: 0.1221\n",
      "Average ROUGE-L: 0.1789\n"
     ]
    }
   ],
   "source": [
    "results_df = evaluate_qa_metrics(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bert_score import BERTScorer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from typing import Dict\n",
    "\n",
    "class QADataFrameEvaluator:\n",
    "    def __init__(self):\n",
    "        self.bert_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        nltk.download('wordnet')\n",
    "        \n",
    "    def semantic_similarity(self, pred: str, ref: str) -> float:\n",
    "        pred_embedding = self.sentence_model.encode([str(pred)])\n",
    "        ref_embedding = self.sentence_model.encode([str(ref)])\n",
    "        similarity = torch.nn.functional.cosine_similarity(\n",
    "            torch.Tensor(pred_embedding), \n",
    "            torch.Tensor(ref_embedding)\n",
    "        )\n",
    "        return float(similarity[0])\n",
    "    \n",
    "    def calculate_meteor(self, pred: str, ref: str) -> float:\n",
    "        return meteor_score([str(ref).split()], str(pred).split())\n",
    "    \n",
    "\n",
    "    def evaluate_dataframe(self, df: pd.DataFrame, reference_col: str = 'Answer', generated_col: str = 'Generated Answers', batch_size: int = 32) -> pd.DataFrame:\n",
    "\n",
    "        semantic_similarities = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(df), batch_size), desc=\"Evaluating answers\"):\n",
    "            batch_df = df.iloc[i:i+batch_size]\n",
    "            \n",
    "            for _, row in batch_df.iterrows():\n",
    "                pred = str(row[generated_col]) if pd.notna(row[generated_col]) else \"\"\n",
    "                ref = str(row[reference_col]) if pd.notna(row[reference_col]) else \"\"\n",
    "                \n",
    "                semantic_similarities.append(self.semantic_similarity(pred, ref))\n",
    "                \n",
    "        avg_scores = {\n",
    "            'Average Semantic Similarity': np.mean(semantic_similarities),\n",
    "        }\n",
    "        \n",
    "        for metric, value in avg_scores.items():\n",
    "            print(f\"{metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[nltk_data] Downloading package wordnet to /home/nikunj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Evaluating answers: 100%|██████████| 2/2 [00:04<00:00,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Semantic Similarity: 0.765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator = QADataFrameEvaluator()\n",
    "evaluator.evaluate_dataframe(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
